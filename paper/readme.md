# 几种常见分词系统的比较
##概述
中文自然语言处理最首要的就是要中文分词了，现在而言效果最好的还是要算crf了，好用的系统可以查看Stanford NLP，但crf的缺点也是明显的，如模型体积大占内存多，速度慢，无法修改等，所以其他的分词系统虽然不如crf效果好，但也不是完全废掉的，所以这里将对如下系统做分析：
1. N-最短路分词（ICTCLAS所用的基层模型）
2. hmm+字典分词 (jieba分词所用的核心模型)
3. TnT分词 (2阶HMM的演化版)
4. crf分词 (这里采用的是crf++工具包，Stanford核心模型)

##模型简介

###N-最短路分词
N-最短路分词是中科院分词系统ICTCLAS中用来做基础分词的核心模块。
核心思想是：根据词典，找出字串中所有可能的词，构造词切分有向无还图，每个词对应图中的一条边。并赋予边一定的权重（这里有机关）。然后求出该图从起点到终点排名前N的最短路径。
N-最短路径并不会直接给出分词的最终结果，而是给出一些可能的结果，再由后续的程序继续处理，最后得出最终结果。
N-最短路径的一个优点就是召回率十分的高，论文中的结果平均在99.5%以上。
[注]：由于个人没有查明ICTCLAS的权重的详细计算方法，所以只是以词频来衡量权重，这里可能欠妥

###hmm+字典分词
hmm+字典分词是jieba分词系统的核心模块，这里就不再介绍hmm模型了，主要分析下特点：hmm模型的一个优点就是具有一定的新词发现能力，这也是Character-Based Generative Model的共有特性，实验中发现hmm模型可以识别一些字典里没有的人名和地名；但hmm模型也存在问题，hmm会错误的将不相关的词连到一起，考量其优点和缺点，我们最后决定放弃hmm的新词发现功能，采用hmm+字典进行分词，用字典去做基础的切分，hmm来处理歧义，取得了还算不错的效果。
[注]：由于放弃了hmm的新词发现能力，所以未登录词只能通过别的方式来解决；实战中发现hmm处理歧义的能力没有想象中那么理想。

###TnT分词
当我们发现一阶hmm不够用时，直观的想可以考虑采用二阶hmm来提高正确率，这里参照的是A00-1031这篇论文的Trigrams'n'Tags（TnT）模型，TnT模型的公式写起来想这样：
	`argmax(ΠP(wordi∣tagi)∗P(tagi∣tagi−1,tagi−2))`
考虑到每个字的标签同样可以加入计算，所以这算是一个TnT的变种，既考虑了已知序列，也考虑了标注序列，它的概率公式写起来像是这样：
	$$$argmax(ΠP((wordi,tagi)∣(wordi−1,tagi−1),(wordi−2,tagi−2)))$$$
看起来像是TnT和普通n-gram的综合，所以效果应该会比以上的模型好一点
[注]：大概还有许多的细节的优化和数据平滑的不到位导致实验结果和论文的结果有差距，以后有时间会继续做调整。
###crf分词
这里采用的是crf++工具包，里面的很多内容还没有完全吃透，只做了性能分析和评测



